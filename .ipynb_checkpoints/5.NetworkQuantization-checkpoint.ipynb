{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Network Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment, the goal is to reduce the size of a deep neural network. This action will provide a lighter and potentially faster model. We will rely on the PyTorch functionalities for quantizing neural networks. The autonomous driving models from the previous assignments will be used for this purpose. \n",
    "\n",
    "In our context, the process of quantization will convert the floating point parameters (32-bit, single precision) to integer parameters.\n",
    "\n",
    "Note that all scripts should be self-contained and executed on *any* machine that has the required libraries installed.\n",
    "\n",
    "The solutions of the assignment can be delivered as Python Notebooks or .py files. The visual results can delivered as pdf- or image-files.\n",
    "\n",
    "**Important**: There is a helpful tutorial on quantization at [https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will make use of the convolutional neural network from the autonomous driving assignment. The goal is to analyze it in terms of of inference time, [FLOPS](https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md) (floating operations), [model size](https://discuss.pytorch.org/t/finding-model-size/130275) (in MB) and accuracy (classification problem). This task does not require training. A pre-trained model from the previous assignments can be employed.\n",
    "\n",
    "*Note*: You may use the library [ptflops](https://pypi.org/project/ptflops/) to determine the number of floating point operations (FLOPS) of your model.\n",
    "\n",
    "*Task Output*: The convolutional model from the autonomous driving assignment should be used in order to compute the execution time, FLOPS, model size and accuracy. For that reason, one function for each metric should be created. The same functions will be later used for evaluating the quantized model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation_learning import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "model = load_model('./models/best_model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptflops in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (0.7.3)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from ptflops) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->ptflops) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->ptflops) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from jinja2->torch->ptflops) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from sympy->torch->ptflops) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to calculate inference time\n",
    "def get_inference_time(model, input_size=(3, 224, 224), num_samples=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = torch.randn(num_samples, *input_size).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            _ = model(inputs[i:i+1])\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_inference = total_time / num_samples\n",
    "    return avg_time_per_inference\n",
    "\n",
    "# Function to calculate FLOPS\n",
    "def get_flops(model, input_size=(3, 224, 224)):\n",
    "    macs, params = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
    "    flops = macs * 2  # FLOPS are 2x the number of MACs\n",
    "    return flops\n",
    "\n",
    "# Function to calculate model size\n",
    "def get_model_size(model_path):\n",
    "    size_in_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "# Function to calculate accuracy (assuming a dataset and evaluation function are available)\n",
    "def get_accuracy(model, dataset_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    accuracy = evaluate_model_accuracy(model, dataset_path, device)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (s): 0.001738\n",
      "FLOPS: 0.11 GFLOPS\n",
      "Model Size (MB): 18.08\n"
     ]
    }
   ],
   "source": [
    "# Define input size based on model requirements\n",
    "input_size = (3, 96, 96)\n",
    "model_path = './models/best_model1.pth'\n",
    "# Compute metrics\n",
    "inference_time = get_inference_time(model, input_size=input_size)\n",
    "flops = get_flops(model, input_size=input_size)\n",
    "model_size = get_model_size(model_path)\n",
    "# accuracy = get_accuracy(model, './data/test_dataset')  # Assuming the path to the dataset\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference Time (s): {inference_time:.6f}\")\n",
    "print(f\"FLOPS: {flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {model_size:.2f}\")\n",
    "# print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. The quantization is static and thus it does not include any training process. \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization).\n",
    "\n",
    "*Task Output*: The weights of the model are float32 variables. They should be converted to int8 (i.e., 8 bit). Then the execution time, FLOPS, model size and accuracy should be computed and compared to the original model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Function to quantize the model\n",
    "def quantize_model(model):\n",
    "    model.eval()\n",
    "    model.qconfig = torch.ao.quantization.get_default_qconfig('x86')\n",
    "    model_fused = torch.ao.quantization.fuse_modules(model,[['conv', 'relu']])\n",
    "    model_prepared = torch.ao.quantization.prepare(model, inplace=True)\n",
    "    model_prepared.eval()\n",
    "    model = torch.ao.quantization.convert(model_prepared, inplace=True)\n",
    "    return model\n",
    "# Quantize the model\n",
    "quantized_model = load_model(model_path)  # Reload the model to avoid inplace changes affecting original model\n",
    "quantized_model = quantize_model(quantized_model)\n",
    "\n",
    "# Save quantized model to file to get its size\n",
    "quantized_model_path = './models/quantized_model.pth'\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flops estimation was not finished successfully because of the following exception:\n",
      "<class 'NotImplementedError'> : Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "Meta: registered at ..\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\n",
      "QuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1928 [kernel]\n",
      "BackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\n",
      "Functionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\n",
      "Named: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\n",
      "AutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\n",
      "AutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\n",
      "AutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\n",
      "AutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\n",
      "AutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\n",
      "AutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\n",
      "AutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\n",
      "AutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\n",
      "AutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\n",
      "Tracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\n",
      "AutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\n",
      "FuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
      "BatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\n",
      "PythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\n",
      "PreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\n",
      "PythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\ptflops\\pytorch_engine.py\", line 64, in get_flops_pytorch\n",
      "    _ = flops_model(batch)\n",
      "        ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1582, in _call_impl\n",
      "    result = forward_call(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\Desktop\\fau\\second semester\\ml lab\\assigment 2\\Imitation-Learning-Autonomous-Driving-Agent\\model.py\", line 99, in forward\n",
      "    x = self.pool(F.relu(self.conv1(x)))\n",
      "                         ^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1532, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1541, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\ao\\nn\\quantized\\modules\\conv.py\", line 468, in forward\n",
      "    return ops.quantized.conv2d(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\Admin\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\_ops.py\", line 854, in __call__\n",
      "    return self_._op(*args, **(kwargs or {}))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "NotImplementedError: Could not run 'quantized::conv2d.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d.new' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
      "\n",
      "Meta: registered at ..\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\n",
      "QuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1928 [kernel]\n",
      "BackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
      "Python: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\n",
      "FuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\n",
      "Functionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\n",
      "Named: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\n",
      "Conjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\n",
      "Negative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\n",
      "ZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\n",
      "ADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\n",
      "AutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\n",
      "AutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\n",
      "AutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\n",
      "AutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\n",
      "AutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\n",
      "AutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\n",
      "AutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\n",
      "AutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\n",
      "AutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\n",
      "Tracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\n",
      "AutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\n",
      "AutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\n",
      "FuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
      "BatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
      "FuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\n",
      "Batched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
      "VmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\n",
      "FuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\n",
      "PythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\n",
      "FuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\n",
      "PreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\n",
      "PythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for *: 'NoneType' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#quantized_inference_time = get_inference_time(quantized_model, input_size=input_size)\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m quantized_flops \u001b[38;5;241m=\u001b[39m get_flops(quantized_model, input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[0;32m      3\u001b[0m quantized_model_size \u001b[38;5;241m=\u001b[39m get_model_size(quantized_model_path)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# quantized_accuracy = get_accuracy(quantized_model, './data/test_dataset')\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 24\u001b[0m, in \u001b[0;36mget_flops\u001b[1;34m(model, input_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_flops\u001b[39m(model, input_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)):\n\u001b[0;32m     23\u001b[0m     macs, params \u001b[38;5;241m=\u001b[39m get_model_complexity_info(model, input_size, as_strings\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, print_per_layer_stat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 24\u001b[0m     flops \u001b[38;5;241m=\u001b[39m macs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m  \u001b[38;5;66;03m# FLOPS are 2x the number of MACs\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m flops\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for *: 'NoneType' and 'int'"
     ]
    }
   ],
   "source": [
    "#quantized_inference_time = get_inference_time(quantized_model, input_size=input_size)\n",
    "quantized_flops = get_flops(quantized_model, input_size=input_size)\n",
    "quantized_model_size = get_model_size(quantized_model_path)\n",
    "# quantized_accuracy = get_accuracy(quantized_model, './data/test_dataset')\n",
    "\n",
    "\n",
    "print(f\"Inference Time (s): {quantized_inference_time:.6f}\")\n",
    "print(f\"FLOPS: {quantized_flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {quantized_model_size:.2f}\")\n",
    "# print(f\"Accuracy: {quantized_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Quantization (optinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized and trained at the same time. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. This type of training is called Quantization-aware training (QAT). \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization)\n",
    "\n",
    "*Note*: This task is optinal and not required to pass the lab course.\n",
    "\n",
    "*Task Output*: The already quantized model from the previous will be use to train conduct the training process. The model should be trained until convergence. Then, then the execution time, FLOPS, model size and accuracy should be computed and compared to the static quantization and the original model.\n",
    "\n",
    "*Important*: Quantization is possible on the eager mode of PyTorch. This requires to install another version of PyTorch.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gymenv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
