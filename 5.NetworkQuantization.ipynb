{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Network Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment, the goal is to reduce the size of a deep neural network. This action will provide a lighter and potentially faster model. We will rely on the PyTorch functionalities for quantizing neural networks. The autonomous driving models from the previous assignments will be used for this purpose. \n",
    "\n",
    "In our context, the process of quantization will convert the floating point parameters (32-bit, single precision) to integer parameters.\n",
    "\n",
    "Note that all scripts should be self-contained and executed on *any* machine that has the required libraries installed.\n",
    "\n",
    "The solutions of the assignment can be delivered as Python Notebooks or .py files. The visual results can delivered as pdf- or image-files.\n",
    "\n",
    "**Important**: There is a helpful tutorial on quantization at [https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will make use of the convolutional neural network from the autonomous driving assignment. The goal is to analyze it in terms of of inference time, [FLOPS](https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md) (floating operations), [model size](https://discuss.pytorch.org/t/finding-model-size/130275) (in MB) and accuracy (classification problem). This task does not require training. A pre-trained model from the previous assignments can be employed.\n",
    "\n",
    "*Note*: You may use the library [ptflops](https://pypi.org/project/ptflops/) to determine the number of floating point operations (FLOPS) of your model.\n",
    "\n",
    "*Task Output*: The convolutional model from the autonomous driving assignment should be used in order to compute the execution time, FLOPS, model size and accuracy. For that reason, one function for each metric should be created. The same functions will be later used for evaluating the quantized model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imitation_learning import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "model = load_model('./models/best_model1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ptflops in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (0.7.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from ptflops) (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (2024.3.1)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from torch->ptflops) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->ptflops) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch->ptflops) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from jinja2->torch->ptflops) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\admin\\anaconda3\\envs\\gymenv\\lib\\site-packages (from sympy->torch->ptflops) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "pip install ptflops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to calculate inference time\n",
    "def get_inference_time(model, input_size=(3, 224, 224), num_samples=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = torch.randn(num_samples, *input_size).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            _ = model(inputs[i:i+1])\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_inference = total_time / num_samples\n",
    "    return avg_time_per_inference\n",
    "\n",
    "# Function to calculate FLOPS\n",
    "def get_flops(model, input_size=(3, 224, 224)):\n",
    "    macs, params = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
    "    flops = macs * 2  # FLOPS are 2x the number of MACs\n",
    "    return flops\n",
    "\n",
    "# Function to calculate model size\n",
    "def get_model_size(model_path):\n",
    "    size_in_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "# Function to calculate accuracy (assuming a dataset and evaluation function are available)\n",
    "def get_accuracy(model, dataset_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    accuracy = evaluate_model_accuracy(model, dataset_path, device)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference Time (s): 0.002951\n",
      "FLOPS: 0.11 GFLOPS\n",
      "Model Size (MB): 18.08\n"
     ]
    }
   ],
   "source": [
    "# Define input size based on model requirements\n",
    "input_size = (3, 96, 96)\n",
    "model_path = './models/best_model1.pth'\n",
    "# Compute metrics\n",
    "inference_time = get_inference_time(model, input_size=input_size)\n",
    "flops = get_flops(model, input_size=input_size)\n",
    "model_size = get_model_size(model_path)\n",
    "# accuracy = get_accuracy(model, './data/test_dataset')  # Assuming the path to the dataset\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference Time (s): {inference_time:.6f}\")\n",
    "print(f\"FLOPS: {flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {model_size:.2f}\")\n",
    "# print(f\"Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. The quantization is static and thus it does not include any training process. \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization).\n",
    "\n",
    "*Task Output*: The weights of the model are float32 variables. They should be converted to int8 (i.e., 8 bit). Then the execution time, FLOPS, model size and accuracy should be computed and compared to the original model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_model(model):\n",
    "    model.eval()\n",
    "    model.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm') \n",
    "    model_fused = torch.ao.quantization.fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])\n",
    "    model_prepared = torch.ao.quantization.prepare(model_fused, inplace=True)\n",
    "    for _ in range(100):\n",
    "        input_tensor = torch.randn(1, 3, 96, 96)\n",
    "        model_prepared(input_tensor)\n",
    "    model = torch.ao.quantization.convert(model_prepared, inplace=True)\n",
    "    return model\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = load_model(model_path)  # Reload the model to avoid inplace changes affecting original model\n",
    "quantized_model = quantize_model(quantized_model)\n",
    "\n",
    "# Save quantized model to file to get its size\n",
    "quantized_model_path = './models/quantized_model.pth'\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at ..\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1928 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m quantized_inference_time \u001b[38;5;241m=\u001b[39m get_inference_time(quantized_model, input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[0;32m      2\u001b[0m quantized_flops \u001b[38;5;241m=\u001b[39m get_flops(quantized_model, input_size\u001b[38;5;241m=\u001b[39minput_size)\n\u001b[0;32m      3\u001b[0m quantized_model_size \u001b[38;5;241m=\u001b[39m get_model_size(quantized_model_path)\n",
      "Cell \u001b[1;32mIn[4], line 14\u001b[0m, in \u001b[0;36mget_inference_time\u001b[1;34m(model, input_size, num_samples)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[1;32m---> 14\u001b[0m         _ \u001b[38;5;241m=\u001b[39m model(inputs[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     15\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     17\u001b[0m total_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\fau\\second semester\\ml lab\\assigment 2\\Imitation-Learning-Autonomous-Driving-Agent\\model.py:92\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 92\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)))\n\u001b[0;32m     93\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[0;32m     94\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, start_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\ao\\nn\\intrinsic\\quantized\\modules\\conv_relu.py:99\u001b[0m, in \u001b[0;36mConvReLU2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     96\u001b[0m     _reversed_padding_repeated_twice \u001b[38;5;241m=\u001b[39m _reverse_repeat_padding(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding)\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, _reversed_padding_repeated_twice,\n\u001b[0;32m     98\u001b[0m                   mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode)\n\u001b[1;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mconv2d_relu(\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_point)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gymenv\\Lib\\site-packages\\torch\\_ops.py:854\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[1;34m(self_, *args, **kwargs)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(self_, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# use `self_` to avoid naming collide with aten ops arguments that\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     \u001b[38;5;66;03m# named \"self\". This way, all the aten ops can be called by kwargs.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[1;32m--> 854\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m self_\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(kwargs \u001b[38;5;129;01mor\u001b[39;00m {}))\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Could not run 'quantized::conv2d_relu.new' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'quantized::conv2d_relu.new' is only available for these backends: [Meta, QuantizedCPU, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nMeta: registered at ..\\aten\\src\\ATen\\core\\MetaFallbackKernel.cpp:23 [backend fallback]\nQuantizedCPU: registered at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv.cpp:1928 [kernel]\nBackendSelect: fallthrough registered at ..\\aten\\src\\ATen\\core\\BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:497 [backend fallback]\nFunctionalize: registered at ..\\aten\\src\\ATen\\FunctionalizeFallbackKernel.cpp:324 [backend fallback]\nNamed: registered at ..\\aten\\src\\ATen\\core\\NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ..\\aten\\src\\ATen\\ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ..\\aten\\src\\ATen\\native\\NegateFallback.cpp:18 [backend fallback]\nZeroTensor: registered at ..\\aten\\src\\ATen\\ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: fallthrough registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:86 [backend fallback]\nAutogradOther: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:53 [backend fallback]\nAutogradCPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:57 [backend fallback]\nAutogradCUDA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:65 [backend fallback]\nAutogradXLA: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:69 [backend fallback]\nAutogradMPS: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:77 [backend fallback]\nAutogradXPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:61 [backend fallback]\nAutogradHPU: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:90 [backend fallback]\nAutogradLazy: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:73 [backend fallback]\nAutogradMeta: registered at ..\\aten\\src\\ATen\\core\\VariableFallbackKernel.cpp:81 [backend fallback]\nTracer: registered at ..\\torch\\csrc\\autograd\\TraceTypeManual.cpp:297 [backend fallback]\nAutocastCPU: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ..\\aten\\src\\ATen\\autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:731 [backend fallback]\nBatchedNestedTensor: registered at ..\\aten\\src\\ATen\\functorch\\LegacyBatchingRegistrations.cpp:758 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\functorch\\VmapModeRegistrations.cpp:27 [backend fallback]\nBatched: registered at ..\\aten\\src\\ATen\\LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ..\\aten\\src\\ATen\\VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ..\\aten\\src\\ATen\\functorch\\TensorWrapper.cpp:202 [backend fallback]\nPythonTLSSnapshot: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ..\\aten\\src\\ATen\\functorch\\DynamicLayer.cpp:493 [backend fallback]\nPreDispatch: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ..\\aten\\src\\ATen\\core\\PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "quantized_inference_time = get_inference_time(quantized_model, input_size=input_size)\n",
    "quantized_flops = get_flops(quantized_model, input_size=input_size)\n",
    "quantized_model_size = get_model_size(quantized_model_path)\n",
    "# quantized_accuracy = get_accuracy(quantized_model, './data/test_dataset')\n",
    "\n",
    "\n",
    "print(f\"Inference Time (s): {quantized_inference_time:.6f}\")\n",
    "print(f\"FLOPS: {quantized_flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {quantized_model_size:.2f}\")\n",
    "# print(f\"Accuracy: {quantized_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Quantization (optinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized and trained at the same time. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. This type of training is called Quantization-aware training (QAT). \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization)\n",
    "\n",
    "*Note*: This task is optinal and not required to pass the lab course.\n",
    "\n",
    "*Task Output*: The already quantized model from the previous will be use to train conduct the training process. The model should be trained until convergence. Then, then the execution time, FLOPS, model size and accuracy should be computed and compared to the static quantization and the original model.\n",
    "\n",
    "*Important*: Quantization is possible on the eager mode of PyTorch. This requires to install another version of PyTorch.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gymenv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
