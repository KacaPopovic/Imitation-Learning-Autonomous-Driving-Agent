{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Neural Network Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this assignment, the goal is to reduce the size of a deep neural network. This action will provide a lighter and potentially faster model. We will rely on the PyTorch functionalities for quantizing neural networks. The autonomous driving models from the previous assignments will be used for this purpose. \n",
    "\n",
    "In our context, the process of quantization will convert the floating point parameters (32-bit, single precision) to integer parameters.\n",
    "\n",
    "Note that all scripts should be self-contained and executed on *any* machine that has the required libraries installed.\n",
    "\n",
    "The solutions of the assignment can be delivered as Python Notebooks or .py files. The visual results can delivered as pdf- or image-files.\n",
    "\n",
    "**Important**: There is a helpful tutorial on quantization at [https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html](https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will make use of the convolutional neural network from the autonomous driving assignment. The goal is to analyze it in terms of of inference time, [FLOPS](https://github.com/facebookresearch/fvcore/blob/main/docs/flop_count.md) (floating operations), [model size](https://discuss.pytorch.org/t/finding-model-size/130275) (in MB) and accuracy (classification problem). This task does not require training. A pre-trained model from the previous assignments can be employed.\n",
    "\n",
    "*Note*: You may use the library [ptflops](https://pypi.org/project/ptflops/) to determine the number of floating point operations (FLOPS) of your model.\n",
    "\n",
    "*Task Output*: The convolutional model from the autonomous driving assignment should be used in order to compute the execution time, FLOPS, model size and accuracy. For that reason, one function for each metric should be created. The same functions will be later used for evaluating the quantized model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "from imitation_learning import *\n",
    "from model import *\n",
    "import numpy as np\n",
    "import torch"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "def load_model(model_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CNN().to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    return model\n",
    "\n",
    "model = load_model('./models/best_model1.pth')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "pip install ptflops"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "from ptflops import get_model_complexity_info\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Function to calculate inference time\n",
    "def get_inference_time(model, input_size=(3, 224, 224), num_samples=100):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = torch.randn(num_samples, *input_size).to(device)\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        for i in range(num_samples):\n",
    "            _ = model(inputs[i:i+1])\n",
    "    end_time = time.time()\n",
    "\n",
    "    total_time = end_time - start_time\n",
    "    avg_time_per_inference = total_time / num_samples\n",
    "    return avg_time_per_inference\n",
    "\n",
    "# Function to calculate FLOPS\n",
    "def get_flops(model, input_size=(3, 224, 224)):\n",
    "    macs, params = get_model_complexity_info(model, input_size, as_strings=False, print_per_layer_stat=False)\n",
    "    flops = macs * 2  # FLOPS are 2x the number of MACs\n",
    "    return flops\n",
    "\n",
    "# Function to calculate model size\n",
    "def get_model_size(model_path):\n",
    "    size_in_mb = os.path.getsize(model_path) / (1024 * 1024)\n",
    "    return size_in_mb\n",
    "\n",
    "# Function to calculate accuracy (assuming a dataset and evaluation function are available)\n",
    "def get_accuracy(model, dataset_path):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    accuracy = evaluate_model_accuracy(model, dataset_path, device)\n",
    "    return accuracy"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Define input size based on model requirements\n",
    "input_size = (3, 96, 96)\n",
    "model_path = './models/best_model1.pth'\n",
    "# Compute metrics\n",
    "inference_time = get_inference_time(model, input_size=input_size)\n",
    "flops = get_flops(model, input_size=input_size)\n",
    "model_size = get_model_size(model_path)\n",
    "# accuracy = get_accuracy(model, './data/test_dataset')  # Assuming the path to the dataset\n",
    "\n",
    "# Print results\n",
    "print(f\"Inference Time (s): {inference_time:.6f}\")\n",
    "print(f\"FLOPS: {flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {model_size:.2f}\")\n",
    "# print(f\"Accuracy: {accuracy:.2f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Static Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. The quantization is static and thus it does not include any training process. \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization).\n",
    "\n",
    "*Task Output*: The weights of the model are float32 variables. They should be converted to int8 (i.e., 8 bit). Then the execution time, FLOPS, model size and accuracy should be computed and compared to the original model.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "def quantize_model(model):\n",
    "    model.eval()\n",
    "    model.qconfig = torch.ao.quantization.get_default_qconfig('fbgemm') \n",
    "    model_fused = torch.ao.quantization.fuse_modules(model, [['conv1', 'relu1'], ['conv2', 'relu2']])\n",
    "    model_prepared = torch.ao.quantization.prepare(model_fused, inplace=True)\n",
    "    for _ in range(100):\n",
    "        input_tensor = torch.randn(1, 3, 96, 96)\n",
    "        model_prepared(input_tensor)\n",
    "    model = torch.ao.quantization.convert(model_prepared, inplace=True)\n",
    "    return model\n",
    "\n",
    "# Quantize the model\n",
    "quantized_model = load_model(model_path)  # Reload the model to avoid inplace changes affecting original model\n",
    "quantized_model = quantize_model(quantized_model)\n",
    "\n",
    "# Save quantized model to file to get its size\n",
    "quantized_model_path = './models/quantized_model.pth'\n",
    "torch.save(quantized_model.state_dict(), quantized_model_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "quantized_inference_time = get_inference_time(quantized_model, input_size=input_size)\n",
    "quantized_flops = get_flops(quantized_model, input_size=input_size)\n",
    "quantized_model_size = get_model_size(quantized_model_path)\n",
    "# quantized_accuracy = get_accuracy(quantized_model, './data/test_dataset')\n",
    "\n",
    "\n",
    "print(f\"Inference Time (s): {quantized_inference_time:.6f}\")\n",
    "print(f\"FLOPS: {quantized_flops / 1e9:.2f} GFLOPS\")\n",
    "print(f\"Model Size (MB): {quantized_model_size:.2f}\")\n",
    "# print(f\"Accuracy: {quantized_accuracy:.2f}%\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Quantization (optinal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task, the parameters of the feed forward model will be quantized and trained at the same time. To reach this goal, experimental functions of PyTorch will be used such as: `torch.quantization`. This type of training is called Quantization-aware training (QAT). \n",
    "\n",
    "Check PyTorch tutorial on [quantization](https://pytorch.org/docs/stable/quantization.html#post-training-static-quantization)\n",
    "\n",
    "*Note*: This task is optinal and not required to pass the lab course.\n",
    "\n",
    "*Task Output*: The already quantized model from the previous will be use to train conduct the training process. The model should be trained until convergence. Then, then the execution time, FLOPS, model size and accuracy should be computed and compared to the static quantization and the original model.\n",
    "\n",
    "*Important*: Quantization is possible on the eager mode of PyTorch. This requires to install another version of PyTorch.\n",
    "\n",
    "*Important*: The scripts should be **self-contained**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gymenv)",
   "language": "python",
   "name": "gymenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
